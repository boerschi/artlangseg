% File acl2014.tex

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{The effects of lexical phonotactics, saturation,
  and frequency skew on segmentability}

\author{Benjamin B{\"o}rschinger \\
    Department of Computing \\
    Macquarie University \\\And
  Robert Daland \\
    Linguistics \\
    UCLA \\
    {\tt benjamin.borschinger@mq.edu.au} \\\And
  Abdellah Fourtassi \\
    LCSP \\
    ENS/EHESS/CNRS \\
    \\
    {\tt \{r.daland\,abdellah.fourtassi\,emmanuel.dupoux\}@gmail.com} \\\And
  Emmanuel Dupoux \\
    LCSP \\
    ENS/EHESS/CNRS }

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Previous works have proposed that the `segmentability' of a language
  depends on its phonotactic structure and can be measured as the
  entropy of licit segmentations of a corpus sample. These proposals
  are tested here by generating artificial languages and measuring
  their segmentability. Maximally permissive and restrictive grammars
  (Pseudo-Berber, Pseudo-Senufo) were used to generate corpus samples 
  in which the lexical saturation and frequency distributions were
  varied parametrically. Lexical saturation tends to cause oversegmentation.
  Interestingly, better segmentation was found for flat frequency
  distributions than for the Zipfian distributions that match the
  prior assumptions of the segmenter. The results show heretofore
  unsuspected nuances of the relationship between phonotactic complexity,
  word length, and word segmentation.
\end{abstract}


\section{Introduction}

Word segmentation is the perceptual process by which infant and adult listeners parse the continuous speech signal into a sequence of discrete, word-like units. The acquisition of word segmentation has been the subject of intense computational scrutiny, where it is typically operationalized as the unsupervised partitioning of a phonemically transcribed, child-directed corpus, and assessed against an orthographically derived gold standard~\cite{GGJ09,DP11,PearlEtAl11}.

Recent years have seen a number of crosslinguistic modeling studies converging on the same conclusion. Across a typologically diverse range of languages (e.g. Arabic, Japanese, Korean, Russian, and Spanish), and across a range of models (lexical, phonotactic, and hybrid), better segmentation is unfailingly predicted for English than for other languages \cite{Fleck08,Daland09,DP11,FourtassiEtAl13,DalandZuraw13}. One possible way to interpret this fact is as evidence of the insufficiency of the models. After all, the developmental trajectory of language acquisition is empirically known to be remarkably robust to cross-linguistic variation in related domains like phonetic categorization~\cite{ref} and meter/prosody/accentuation~\cite{ref}. On the other hand, there is considerably less cross-linguistic work on word segmentation in infants, and the available work is consistent with the alternative hypothesis, e.g. later segmentation of function words by French-acquiring infants than English-acquiring ones.~\cite{ref}. On balance, then, existing work suggests that languages genuinely differ in segmentability (i.e. the predicted ease/accuracy with which infants might learn to segment speech in that language).

Recently, it was proposed in two independent studies that cross-linguistic differences in segmentability may arise in part or in whole from language-specific phonotactics. Daland and Zuraw~\shortcite{DalandZuraw13} investigated the segmentability of Korean using a phonotactic segmentation model~\cite{DP11}. Korean possesses many edge-sensitive phonological processes, so one might naturally think that segment sequences would be distinct across word boundaries versus within words; the authors found that overall, this didn't hold. For example, vowel-vowel (hiatus) sequences are tolerated within words in Korean, while vowels are also common both word-initially and in word-final inflections; thus hiatus is not a good boundary cue in Korean (though it is in English). Fourtassi et al.~\shortcite{FourtassiEtAl13} investigated the segmentability of Japanese using a lexical segmentation model~\cite{GGJ09}. Japanese lexical items often consist of multiple sub-parts, which are themselves phonologically acceptable as words, e.g. \textit{katakana} `framentary characters'; cf. \textit{kana} `character'. Fourtassi et al. found that the comparatively poor segmentability of Japanese was strongly predicted by the `normalized segmentation entropy' of a corpus, the average (per-character) entropy over all licit segmentations of a corpus given the gold standard lexicon. Both Daland and Zuraw and Fourtassi et al. speculated that the ultimate source of the poor segmentability of these languages lay in their restrictive phonotactics.

The present study seeks to test this proposal by applying an Adaptor Grammar-based segmentation model~\cite{GGJ09} to artificially generated corpora. Artificial corpora are used here for the same reason that psycholinguistic experiments used carefully controlled stimuli rather than randomly sampled natural production: natural language corpora vary in a number of ways, arising from complex and potentially not understood relationships between interacting language properties. One example of a property that must be controlled is the word frequency distribution. It stands to reason that the word frequency distribution might differ between, e.g., Japanese and English for essentially syntactic reasons. Overt subjects are normally obligatory in English, so subject and object pronouns tend to make up a large noticable proportion of the total frequency. In contrast, discourse-salient subjects and objects are typically omitted in Japanese, so pronoun-like elements will likely make up a smaller proportion of the total frequency in Japanese corpora. As Adaptor Grammars adopt particular assumptions about the word frequency distribution of a language, it stands to reason that this language property will affect the segmentability of an English or a Japanese corpus, quite apart from the phonotactic properties. By generating artificial corpora, it is possible to vary the phonotactic properties while holding the word frequency distribution constant. It is also possible to investigate the effect of the word frequency distribution. In fact, this is exactly what the present paper does.

Two artificial languages were created, representing extremes of phonotactic permissiveness and restrictiveness. Pseudo-Senufo is a strict CV grammar -- words must begin with a consonant and end with a vowel; no consonant-consonant or vowel-vowel sequences are permitted; and otherwise every possible sequence of $(CV)^*$ is grammatical. Pseudo-Berber has no phonotactic constraints whatsoever; every possible sequence of consonants and vowels is grammatical. Both grammars were defined over the same segmental inventory, intended to represent a fairly minimal but cross-linguistically typical inventory (see Methods). In addition to the language manipulation (Berber vs. Senufo) and the word frequency distribution manipulation (described in more detail below), a final manipulation was to impose varying degrees of penalty on word length. This was done for a purely practical reason, to prevent overly long words from dominating the artificial lexicons; although the length penalty manipulation proved interesting.



\section{Methods}

this section describes what we actually did

\section{Results}

this section describes what we found

\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|cccccc|}
  \hline
  Language & *Struct & brent & zipf16 & zipf12 & linear & flat & point \\
  \hline
  Senufo & 1 & .85 & .65 & .83 & .66 & .58 & .13 \\
  Senufo & 2 & .87 & .65 & .83 & .78 & .76 & .13 \\
  Senufo & 3 & .83 & .50 & .71 & .91 & .91 & .13 \\
  \hline
  Berber & 1 & .93 & .85 & .91 & .70 & .62 & .20 \\
  Berber & 2 & .95 & .75 & .92 & .78 & .73 & .21 \\
  Berber & 3 & .95 & .62 & .87 & .93 & .91 & .20 \\
  \hline
\end{tabular}
\end{center}
\caption{\label{Results.}}
\end{table}

\section{Discussion}

this section says what it all means

If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
can use the command \verb|\newcite| to get ``author (year)'' citations.


% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2014}

\begin{thebibliography}{}

\bibitem[\protect\citename{Pearl, Goldwater, and Steyvers, 2011}]{PearlEtAl11}
Lisa Pearl, Sharon Goldwater, and Mark Steyvers.
\newblock 2011.
\newblock Online learning mechanisms for Bayesian models of word segmentation.
\newblock {\em Research on Language and Computation}, 8(2):107--132.
\newblock DOI 10.1007/s11168-011-9074-5.

\bibitem[\protect\citename{Goldwater, Griffiths, and Johnson, 2009}]{GGJ09}
Sharon Goldwater, Thomas~L. Griffiths, and Mark Johnson.
\newblock 2009.
\newblock A Bayesian framework for word segmentation:
    Exploring the effects of context.
\newblock {\em Cognition}, 112:21--54.

\bibitem[\protect\citename{Daland and Pierrehumbert, 2011}]{DP11}
Robert Daland and Janet~B. Pierrehumbert.
\newblock 2011.
\newblock Learning diphone-based segmentation.
\newblock {\em Cognitive Science}, 35(1):119--155.

\bibitem[\protect\citename{Fleck, 2008}]{Fleck08}
Margaret~M. Fleck.
\newblock 2008.
\newblock Lexicalized phonotactic word-segmentation.
\newblock {\em Proceedings of the 45th ACL}, HLT:130--138.

\bibitem[\protect\citename{Daland, 2009}]{Daland09}
Roert Daland.
\newblock 2009.
\newblock {\em Word segmentation, word recognition, and word learning:
    a computational model of first language acquisition.}
\newblock PhD Dissertation, Northwestern University.

\bibitem[\protect\citename{Fourtassi et al., 2013}]{FourtassiEtAl2013}
Abdellah Fourtassi, Benjamin Borschinger, Mark Johnson, and Emmanuel Dupoux.
\newblock 2013.
\newblock whyisenglishsoeasytosegment?
\newblock {\em Proceedings of the Workshop on Cognitive Modeling and
     Computational Linguistics, August 8, 2013, Sofia, Bulgaria}, CMCL:1--10.

\bibitem[\protect\citename{Daland and Zuraw, 2013}]{DalandZuraw2013}
Robert Daland and Kie Zuraw.
\newblock 2013.
\newblock Does Korean defeat phonotactic word segmentation?
\newblock {\em Proceedings of the 51st ACL in Sofia, Bulgaria.}.

%\bibitem[\protect\citename{what appears in the text}]{how you label it}
%Authors.
%\newblock date.
%\newblock title
%\newblock {\em to italicize}, deets.


\end{thebibliography}

\end{document}
