% File acl2014.tex

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{multirow}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{phonetic}
\usepackage{graphicx}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{The effects of lexical phonotactics, saturation,
  and frequency skew on segmentability}

\author{Benjamin B{\"o}rschinger \\
    Department of Computing \\
    Macquarie University \\\And
  Robert Daland \\
    Linguistics \\
    UCLA \\
    {\tt benjamin.borschinger@mq.edu.au} \\\And
  Abdellah Fourtassi \\
    LCSP \\
    ENS/EHESS/CNRS \\
    \\
    {\tt \{r.daland\,abdellah.fourtassi\,emmanuel.dupoux\}@gmail.com} \\\And
  Emmanuel Dupoux \\
    LCSP \\
    ENS/EHESS/CNRS }

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Previous works have proposed that the `segmentability' of a language
  depends on its phonotactic structure and can be measured as the
  entropy of licit segmentations of a corpus sample. These proposals
  are tested here by generating artificial languages and measuring
  their segmentability. Maximally permissive and restrictive grammars
  (Pseudo-Berber, Pseudo-Senufo) were used to generate corpus samples 
  in which the lexical saturation and frequency distributions were
  varied parametrically. %Lexical saturation tends to cause oversegmentation.
%  Interestingly, better segmentation was found for flat frequency
%  distributions than for the Zipfian distributions that match the
%  prior assumptions of the segmenter.
  The results show heretofore  unsuspected nuances of the relationship
  between phonotactic complexity, word length, and word segmentation.
\end{abstract}


\section{Introduction}

Word segmentation is the perceptual process by which infant and adult listeners parse the continuous speech signal into a sequence of discrete, word-like units. The acquisition of word segmentation has been the subject of intense computational scrutiny, where it is typically operationalized as the unsupervised partitioning of a phonemically transcribed, child-directed corpus, and assessed against an orthographically derived gold standard~\cite{Goldwater09a,Daland11a,Pearl10b}.

A number of modeling studies indicates that across a typologically diverse range of languages (e.g. Arabic, Japanese, Korean, Russian, and Spanish), and across a range of models (lexical, phonotactic, and hybrid), better segmentation is unfailingly found for English than for other languages \cite{Fleck08a,Daland09a,Daland11a,Fourtassi13a,Daland13a}. The empirical data bearing on this point is sparse, and occasionally conflicting \cite{Nazzi06a,Nazzi14a}. Therefore, while current models may be insufficient, it seems worth considering the possibility that languages genuinely differ in their intrinsic ease of segmentation.

% While this may be due to the general insufficiency of current models, there is also some evidence in the available developmental work that studies infant word segmentation cross-lingually for noticeable differences in segmentation of young infants for different languages.~\cite{Nazzi06a}. On balance, then, existing work suggests that languages genuinely differ in segmentability (i.e. the predicted ease/accuracy with which infants might learn to segment speech in that language).

%Recently, it was proposed in two independent studies that cross-linguistic differences in segmentability may arise in part or in whole from language-specific phonotactics. 
Along those lines, Daland and Zuraw~\shortcite{Daland13a} investigated the segmentability of Korean using a phonotactic segmentation model~\cite{Daland11a}, finding that contrary to expectation, Korean's many edge-sensitive phonological processes did not help word segmentation. %For example, vowel-vowel (hiatus) sequences are tolerated within words in Korean, while vowels are also common both word-initially and in word-final inflections. Thus hiatus is not a good boundary cue in Korean (though it is in English).
Fourtassi et al.~\shortcite{Fourtassi13a} compared the segmentability of Japanese and English using lexical segmentation models~\cite{Goldwater09a,Johnson09a} and argued that the comparatively poor segmentability of Japanese was strongly predicted by the `normalized segmentation entropy' of a corpus, the average (per-character) entropy over all licit segmentations of a corpus given the gold standard lexicon. Both Daland and Zuraw and Fourtassi et al. speculated that the poor segmentability of these languages bears close connection to their restrictive phonotactics.

The present study seeks to test this proposal by applying the popular Unigram model~\cite{Brent99a,Goldwater07c,Goldwater09a} to artificially generated corpora. Artificial corpora are used here for the same reason that psycholinguistic experiments use carefully controlled stimuli rather than randomly sampled natural production: natural language corpora vary in a number of ways, arising from complex and potentially not understood relationships between interacting language properties. %One example of a property that must be controlled is the word frequency distribution. 
For example, it stands to reason that the word frequency distribution might differ between, e.g., Japanese and English for essentially syntactic (and morphological) reasons. %Overt subjects are normally obligatory in English, so subject and object pronouns tend to make up a large noticable proportion of the total frequency. %In contrast, discourse-salient subjects and objects are typically omitted in Japanese, so pronoun-like elements will likely make up a smaller proportion of the total frequency in Japanese corpora. 
%As lexical segmentation models like the Unigram model embody particular assumptions about the word frequency distribution of a language, it stands to reason that this language property will affect segmentability by such a model, quite apart from the phonotactic properties. 
By generating artificial corpora, it is possible to vary one property (phonotactics, frequency distributions, word lengths) while holding other factors constant, controlling (to a certain extent) the impact of these additional factors. In fact, this is exactly what the present paper does.

 %This was done for a purely practical reason, to prevent overly long words from dominating the artificial lexicons; although the length penalty manipulation proved interesting.

\section{Methods}
\vspace*{-5pt}
Two artificial languages were created, representing extremes of phonotactic permissiveness and restrictiveness. Pseudo-Senufo is a strict CV grammar -- words must begin with a consonant and end with a vowel.%; no consonant-consonant or vowel-vowel sequences are permitted; and otherwise every possible sequence of $(CV)^*$ is grammatical.
Pseudo-Berber has no phonotactic constraints whatsoever -- every possible sequence of consonants and vowels is grammatical. Both grammars were defined over the same segmental inventory, intended to represent a fairly minimal but cross-linguistically typical inventory (see Methods). In addition to the language manipulation (Berber vs. Senufo) and the word frequency distribution manipulation (described in more detail below), a final manipulation was to impose varying degrees of penalty on word length.
 
\subsection{Phonotactic grammars}

The alphabet $\Sigma$ used here was intended to represent a small but standard segmental inventory, including plain stops at 3 major places of articulation [ptk], corresponding fricatives [fsx] and nasals [mn\engma], basic liquids [lr] and glides [jw], and a standard 5-vowel inventory [aeiou]. These segments were assigned standard phonological feature values, e.g. [f] is [-son,+cont,+lab], while [a] is [+syl,+low]. %In the implementation, strings are space-separated, e.g. `s t r i \engma'.
A \textit{phonotactic grammar} $\mu$ over an alphabet $\Sigma$ defines a probability distribution over $\Sigma^*$, i.e. it is a function $\mu : \Sigma^* \rightarrow [0,1]$ which assigns a probability to every string in $\Sigma^*$ (the set of finite strings over $\Sigma$). Here, the phonotactic grammar was defined using constraints stated over $n$-gram feature matrices, following the format of regular expressions. For example, the constraint `\textasciicircum[+syl]' penalizes words which begin with a vowel (often called \textsc{Onset} in the phonological literature). Each constraint is associated with a weight, and the `harmony' of a string is the weighted sum of its constraint violations. This defines a log-linear model, from which the probability of a string can be calculated exactly ~\cite{Hayes08a}. The maximum string length was 6 in the present paper. The grammar of Pseudo-Senufo contains an \textsc{Onset} constraint `\textasciicircum[+syl]', a \textsc{NoCoda} constraint `[-syl]\$', a constraint banning consonant clusters `[-syl] [-syl]', and a constraint banning vowel hiatus `[+syl] [+syl]'. Each of these was given a very strong weight (-25), sufficient to categorically rule out violating forms, enforcing a strict CV syllable structure. In addition, there is a \textsc{*Struct} constraint `[]' which penalizes words according to how many segments they contained. This constraint was given a smaller weight (between -1 and -5), which was manipulated to control the average word length. The grammar of Pseudo-Berber only contains \textsc{*Struct}, allowing every possible sequence of segments. 

\subsection{Word frequency distributions}

The distribution of patterns in their input plays a crucial role for the outcome of learning for statistical models. Side-stepping the complex interactions between semantics, syntax and morphology that give rise to word distributions in real languages we generate artificial corpora mimicking, to varying degrees, the token distribution of natural language. As a blue-print, we take the type and token statistics from the Brent-Bernstein-Ratner corpus~\cite{Brent99a}, containing a specific distribution for 1365 distinct types and 33347 tokens. From this, we can generate artificial corpora that match this distribution by first sampling 1365 distinct types from the phonotactic grammars described above and then generating a random permutation of 33347 tokens that match the original distribution perfectly.\footnote{To generate utterance boundaries, we assume a stopping probability of $\frac{1}{3}$, resulting in corpora with identical numbers of types and tokens but slightly differing numbers of utterances.} We refer to corpora that follow this distribution as \textsc{bbr}, and also experiment with a distribution in which all types occur with (roughly) the same frequency (\textsc{flat}) and an artificial power-law distribution that exhibits a long-tail distribution similar yet different to that of an actual natural language sample (\textsc{zipf}). 

\subsection{Unigram word segmentation}

The Unigram model for word segmentation is a simple generative model for sequences of words, built on the non-parametric Dirichlet Process (DP) prior. For reasons of space, we refer the reader to \cite{Goldwater07c} for details, providing only a brief explanation. The model tries to learn a probabilistic lexicon, that is, a distribution over words, that can account for the observed unsegmented data in a compact fashion through a small number of reusable items. It requires a prior distribution over admissible words, and here we follow prior work in choosing the ``Monkey-model'' generater~\cite{Brent99a,Goldwater07c}. This prior assigns geoemtrically decaying probabilities to arbitrary sequences of any length; we deliberately opt for such a naive prior to focus on the information conveyed by the input, rather than any linguistically motivated prior biases learners might bring to the task. When applied to a corpus of unsegmented utterances, it tries to identify a compact analysis of the corpus which defines a probabilistic lexicon. Crucially, the DP prior favours distributions that exhibit a Zipfian rich-get-richer dynamic with relatively few high-frequency items and a long tail of low-frequency items. Also, the model assumes that words within a segmentation are independent, hence the name ``Unigram model''. While this has been shown to result in undersegmentation of frequent collocations, we chose the Unigram model because the questions of this paper are by and large independent of this known shortcoming. In particular, our artificial languages fit the Unigram assumption that is obviously inadequate for natural languages by construction, allowing us to focus on phonotactics and word-frequency related issues.

\section{Results}
\vspace*{-5pt}
Our experimental paradigm follows closely that of \cite{Johnson09a}. For each of our artificial corpora we run four independent Markov Chains for the Unigram model for 1000 iterations using the Adaptor Grammar software~\cite{Johnson07c}, calculating a marginal maximum a posteriori segmentation for each utterance from the samples collected across all 4 chains during the last 200 iterations. We evaluate segmentation accuracy by calculating the token f-score according to the gold boundaries in the artificial corpora. Token f-score is the harmonic mean of token precision, i.e. the fraction of correctly identified tokens in the posited segmentation over all posited tokens, and token recall, i.e. the fraction of correctly recovered tokens over all tokens in the gold standard. 

The results are given in Table~\ref{Results.}. Overall, pseudo-Berber seems to be segmented better than pseudo-Senufo, with its best token f-scores for \textsc{bbr} and \textsc{zipf} being 95\% and 92\% respectively, compared to 87\% and 83\% for pseudo-Senufo. Only in the highly unnatural \textsc{flat} condition, pseudo-Senufo peaks with 97\%, with pseudo-Berber only reaching 94\%. There also is a clear impact of the \textsc{*Struct} constraint that governs the average word lengths, although the importance of this constraint varies considerably. For \textsc{bbr}, pseudo-Senufo yields a token f-score of 80\%+x for all values of \textsc{*Struct} whereas pseudo-Berber drops to 59\% for \textsc{*Struct}=5 but yields a considerably higher token f-score than pseudo-Senufo for \textsc{*Struct}<4. The \textsc{zipf} condition seems to favour lower values of \textsc{*Struct} equally for both pseudo-Berber and pseudo-Senufo; again, pseudo-Berber attains the worst token f-score of 45\% (compared to 67\% for pseudo-Senufo) for \textsc{*Struct}=5. In contrast, \textsc{flat} looks like a mirror of \textsc{zipf}, yielding exceptionally high token f-scores for both pseudo-Berber and pseudo-Senufo for high values of \textsc{*Struct} and bad performance for smaller ones.

%NOTE: \textit values are still running but seem to have converged, hence I included them to get an idea of what the complete table will look like
%\begin{table*}[t!]
%\begin{center}
%\begin{tabular}{|c||c|cccccc|}
%  \hline
%  Language & *Struct & brent & zipf16 & zipf12 & linear & flat & point \\
%  \hline
%  Senufo & 1 & .85 & .65 & .83 & .66 & .58 & .13 \\
%  Senufo & 2 & .87 & .65 & .83 & .78 & .76 & .13 \\
%  Senufo & 3 & .83 & .50 & .71 & .91 & .91 & .13 \\
%  Senufo & 4 & .83 & .50 & .68 & .95 & .96 & .13 \\
%  Senufo & 5 & .83 & .50 & .67 & \textit{.95} & \textit{.97} & .14 \\
%  \hline
%  Berber & 1 & .93 & .85 & .91 & .70 & .62 & .20 \\
%  Berber & 2 & .95 & .75 & .92 & .78 & .73 & .21 \\
%  Berber & 3 & .95 & .62 & .87 & .93 & .91 & .20 \\
%  Berber & 4 & .80 & .42 & .62 & .91 & .94 & .20 \\
%  Berber & 5 & \textit{.59} & .33 & \textit{.45} & \textit{.85} & \textit{.91} & .19 \\
%  \hline
%\end{tabular}
%\end{center}
%\caption{\label{Results.}}
%\end{table*}

\begin{table}[t!]
\begin{center}
%\begin{tabular}{|c|c|c|c||c|c|c||c|c|c||c|c|c|}
%\cline{5-13} 
%\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{3}{c||}{brent} & \multicolumn{3}{c||}{zipf} & \multicolumn{3}{c|}{flat}\tabularnewline
%\hline 
%S & L & SAT & LA & TF & NSE & TL & TF & NSE & TL & TF & NSE & TL\tabularnewline
%\hline 
%\hline 
%\multirow{2}{*}{1} & s & 0.01 & .03 & .85  & .0 & 5.83 & .83  & .0 & 5.87 & .58  & .0 & 5.75\tabularnewline
%\cline{2-13} 
% & b & 0.00 & .0 & .93 & .0 & 5.84 & .91 & .0 & 5.86 & .62  & .0 & 5.80\tabularnewline
%\hline 
%\hline 
%\multirow{2}{*}{2} & s & 0.33 & 1.03 & .87 & .01 & 4.35 & .83 & .0 & 4.94 & .76 & .0 & 4.95\tabularnewline
%\cline{2-13} 
% & b & 0.00 & .05 & \textbf{.95} & .0 & 5.40 & \textbf{.92} & .0 & 5.48 & .73 & .0 & 5.36\tabularnewline
%\hline 
%\hline 
%\multirow{2}{*}{3} & s & 0.88 & .99 & .83 & .06 & 3.04 & .71 & .04 & 3.21 & .91 & .01 & 4.20\tabularnewline
%\cline{2-13} 
% & b & 0.36 & 1.59 & \textbf{.95} & .02 & 3.54 & .87 & .03 & 2.97 & .91 & .01 & 3.95\tabularnewline
%\hline 
%\hline 
%\multirow{2}{*}{4} & s & 0.98 & .96 & .83 & .08 & 2.81 & .68 & .08 & 2.62 & .96 & .01 & 3.96\tabularnewline
%\cline{2-13} 
% & b & 0.90 & 1.77 & .80 & .18 & 2.19 & .62 & .24 & 1.96 & .94 & .02 & 3.10\tabularnewline
%\hline 
%\hline 
%\multirow{2}{*}{5} & s & 1.00 & .95 & .83 & .09 & 2.78 & .67 & .08 & 2.60 & \textbf{.97}  & .01 & 3.90\tabularnewline
%\cline{2-13} 
% & b & 0.99 & 1.75 & .59  & .34 & 1.85 & .45 & .39 & 1.67 & .91  & .03 & 2.81\tabularnewline
%\hline 
%\end{tabular}
\scalebox{0.77}{
\begin{tabular}{|c|c||c|c|c||c|c|c||c|c|c|}
\cline{3-11} 
\multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{3}{c||}{\textsc{bbr}} & \multicolumn{3}{c||}{\textsc{zipf}} & \multicolumn{3}{c|}{\textsc{flat}}\tabularnewline
\hline 
S & L & TF & SE & TL & TF & SE & TL & TF & SE & TL\tabularnewline
\hline 
\hline 
\multirow{2}{*}{1} & s & .85  & .0 & 5.83 & .83  & .0 & 5.87 & .58  & .0 & 5.75\tabularnewline
\cline{2-11} 
 & b & .93 & .0 & 5.84 & .91 & .0 & 5.86 & .62  & .0 & 5.80\tabularnewline
\hline 
\hline 
\multirow{2}{*}{2} & s & .87 & .01 & 4.35 & .83 & .0 & 4.94 & .76 & .0 & 4.95\tabularnewline
\cline{2-11} 
 & b & \textbf{.95} & .0 & 5.40 & \textbf{.92} & .0 & 5.48 & .73 & .0 & 5.36\tabularnewline
\hline 
\hline 
\multirow{2}{*}{3} & s & .83 & .06 & 3.04 & .71 & .04 & 3.21 & .91 & .01 & 4.20\tabularnewline
\cline{2-11} 
 & b & \textbf{.95} & .02 & 3.54 & .87 & .03 & 2.97 & .91 & .01 & 3.95\tabularnewline
\hline 
\hline 
\multirow{2}{*}{4} & s & .83 & .08 & 2.81 & .68 & .08 & 2.62 & .96 & .01 & 3.96\tabularnewline
\cline{2-11} 
 & b & .80 & .18 & 2.19 & .62 & .24 & 1.96 & .94 & .02 & 3.10\tabularnewline
\hline 
\hline 
\multirow{2}{*}{5} & s & .83 & .09 & 2.78 & .67 & .08 & 2.60 & \textbf{.97}  & .01 & 3.90\tabularnewline
\cline{2-11} 
 & b & .59  & .34 & 1.85 & .45 & .39 & 1.67 & .91  & .03 & 2.81\tabularnewline
\hline 
\end{tabular}}
\end{center}
\caption{\label{Results.}Experimental results, grouped by different values of the \textsc{*Struct} constraint (S column) and language (L column, with ``s'' for pseudo-Senufo and ``b'' for pseudo-Berber). ``TF'' stands for token f-score, ``SE'' for normalized segmentation entropy and ``TL'' for the average gold token length for that condition.}	
\end{table}

\section{Discussion}
\vspace*{-5pt}
The results raise several interesting questions. First, it is surprising that the phonotactically much less constrained pseudo-Berber seems to be, on average, more easily to segment, yielding the highest token f-score for both the natural and the artificial Zipfian condition. Even if the segmentation model does not explicitly consider phonotactic cues, a natural expectation would have been that regular phonotactics yield a more regular pattern of lexical forms that would make identification of repeating units easier. There is also considerable variability of segmentability as a function of the frequency distribution and the average word length, with the overall best segmentability achieved for a highly unnatural flat distribution with predominantly short words. Curiously, the flat distribution violates the prior expectation of the DP based Unigram model much stronger than either the natural or the artificial Zipfian condition, nevertheless resulting in the overall highest token f-scores. The first observation lends itself to an explanation along the lines of \cite{Fourtassi13a}: we calculate the normalized segmentation entropy (SE) for the artificial corpora, a measure of how much ambiguity with respect to word segmentation remains if the gold lexicon is provided to the learner, 0 indicating no uncertainty at all and 1.0 indicating complete uncertainty; we refer the reader to \cite{Fourtassi13a} for a more detailed explanation. For both \textsc{bbr} and \textsc{zipf}, SE provides a plausible explanation for the variation in segmentability. Note that for pseudo-Berber, SE tends to be lower than for pseudo-Senufo for \textsc{*Struct}-values up to roughly 3, and these are exactly the conditions where pseudo-Berber attains higher token f-scores. The trend reverses for \textsc{*Struct}$>$3, again as would be predicted by simply looking at the SE. To understand why SE varies the way it does, note that for any given word-length, there are strictly less possible word types for pseudo-Senufo than for pseudo-Berber, making the reuse of short words as subparts of longer words necessary to a much larger extent than for pseudo-Berber. In particular, there is a relatively small set of CV-sequences (65 for the inventory used here) that, by construction, have to occur in every possible word and might very well be words themselves, thus leading to inherent ambiguity. Unless forced to rely almost exclusively on short words (corresponding to \textsc{*Struct}$>$3), pseudo-Berber has a larger lexical space at its disposal, leading to lower ambiguity and consequently more successful unsupervised segmentation. Yet, the absence of any constraint on possible word-forms turns into a disadvantage for settings where words tend to be very short --- in such a setting, pseudo-Berber suffers from admitting with a high frequency single segment ``words'' that lead to considerably higher SEs than for pseudo-Senufo and, consequently, considerably lower segmentation accuracy. While this explanation pertains directly to the artificial setting of this paper, it also provides strong support to the previous suggestions by \cite{Daland13a} and \cite{Fourtassi13a} that restrictive phonotactics can have a \emph{negative} impact on segmentability, abstracting away from other possible confounding factors that are impossible to control for when working with natural languages.

The pattern for the \textsc{flat} condition seems to reflect that for relatively short words, a flat frequency distribution which is dominated by no single type but spreads its probability mass ``fairly'' seems to lead to ideal segmentability, arguably because there is just enough evidence for every individual type. Crucially, the absence of any types that occur with exceptionally high frequency prevent the model to improperly segment these units out of lower frequency larger words, precisely the problem we identified for \textsc{bbr} and \textsc{zipf} and pseudo-Berber with high \textsc{*Struct}-values. Once words tend to get ``too long'', however, the absence of high frequency types prevents the model to ``break into'' the segmentation by identifying frequent items, resulting in the reverse of the situation we find for \textsc{bbr} and \textsc{zipf}. We also note that across the three kinds of frequency distributions studied here, the ``natural'' Brent distribution is the most stable across varying values of \textsc{Struct}, in particular for pseudo-Senufo. This indicates that actual natural language frequency distributions do indeed facilitate tasks such as word segmentations and are, in a sense to be more precise in future work, more robust to variations in word lengths than artificial distributions, consistent with recent experimental work~\cite{Kurumada13a}.

\section{Conclusion}
\vspace*{-5pt}
We used artificial languages to support the idea that and provide an explanation of why more rigid phonotactics negatively impact statistical word segmentation models. We also found some evidence that natural frequency distributions are more robust to variations in word lengths than artificial created ones. Our use of artificial languages instead of real languages allows us to at least partially control for many of the complex interaction natural language exhibits, thus directly addressing the role played by phonotactics. While ultimately, we want to understand how infants segment real languages, we believe to have shown how artificial languages can help in better understanding our computational models and addressing specific hypotheses, steps that need to be taken to design more adequate models in future research.
% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{acl}


\end{document}
