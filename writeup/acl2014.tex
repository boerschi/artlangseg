% File acl2014.tex

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{phonetic}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{The effects of lexical phonotactics, saturation,
  and frequency skew on segmentability}

\author{Benjamin B{\"o}rschinger \\
    Department of Computing \\
    Macquarie University \\\And
  Robert Daland \\
    Linguistics \\
    UCLA \\
    {\tt benjamin.borschinger@mq.edu.au} \\\And
  Abdellah Fourtassi \\
    LCSP \\
    ENS/EHESS/CNRS \\
    \\
    {\tt \{r.daland\,abdellah.fourtassi\,emmanuel.dupoux\}@gmail.com} \\\And
  Emmanuel Dupoux \\
    LCSP \\
    ENS/EHESS/CNRS }

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Previous works have proposed that the `segmentability' of a language
  depends on its phonotactic structure and can be measured as the
  entropy of licit segmentations of a corpus sample. These proposals
  are tested here by generating artificial languages and measuring
  their segmentability. Maximally permissive and restrictive grammars
  (Pseudo-Berber, Pseudo-Senufo) were used to generate corpus samples 
  in which the lexical saturation and frequency distributions were
  varied parametrically. Lexical saturation tends to cause oversegmentation.
  Interestingly, better segmentation was found for flat frequency
  distributions than for the Zipfian distributions that match the
  prior assumptions of the segmenter. The results show heretofore
  unsuspected nuances of the relationship between phonotactic complexity,
  word length, and word segmentation.
\end{abstract}


\section{Introduction}

Word segmentation is the perceptual process by which infant and adult listeners parse the continuous speech signal into a sequence of discrete, word-like units. The acquisition of word segmentation has been the subject of intense computational scrutiny, where it is typically operationalized as the unsupervised partitioning of a phonemically transcribed, child-directed corpus, and assessed against an orthographically derived gold standard~\cite{Goldwater09a,Daland11a,Pearl10b}.

Recent years have seen a number of crosslinguistic modeling studies converging on the same conclusion. Across a typologically diverse range of languages (e.g. Arabic, Japanese, Korean, Russian, and Spanish), and across a range of models (lexical, phonotactic, and hybrid), better segmentation is unfailingly found for English than for other languages \cite{Fleck08a,Daland09a,Daland11a,Fourtassi13a,Daland13a}. One possible way to interpret this fact is as evidence of the insufficiency of the models. After all, the developmental trajectory of language acquisition is empirically known to be remarkably robust to cross-linguistic variation in related domains like phonetic categorization~\cite{ref} and meter/prosody/accentuation~\cite{ref}. On the other hand, there is considerably less cross-linguistic work on word segmentation in infants, and the available work is consistent with the alternative hypothesis, e.g. later segmentation of function words by French-acquiring infants than English-acquiring ones.~\cite{ref}. On balance, then, existing work suggests that languages genuinely differ in segmentability (i.e. the predicted ease/accuracy with which infants might learn to segment speech in that language).

Recently, it was proposed in two independent studies that cross-linguistic differences in segmentability may arise in part or in whole from language-specific phonotactics. Daland and Zuraw~\shortcite{Daland13a} investigated the segmentability of Korean using a phonotactic segmentation model~\cite{Daland11a}. Korean possesses many edge-sensitive phonological processes, so one might naturally think that segment sequences would be distinct across word boundaries versus within words; the authors found that overall, this didn't hold. For example, vowel-vowel (hiatus) sequences are tolerated within words in Korean, while vowels are also common both word-initially and in word-final inflections. Thus hiatus is not a good boundary cue in Korean (though it is in English). Fourtassi et al.~\shortcite{Fourtassi13a} investigated the segmentability of Japanese using a lexical segmentation model~\cite{Goldwater09a}. Japanese lexical items often consist of multiple sub-parts, which are themselves phonologically acceptable as words, e.g. \textit{katakana} `fragmentary characters'; cf. \textit{kana} `character'. Fourtassi et al. found that the comparatively poor segmentability of Japanese was strongly predicted by the `normalized segmentation entropy' of a corpus, the average (per-character) entropy over all licit segmentations of a corpus given the gold standard lexicon. Both Daland and Zuraw and Fourtassi et al. speculated that the ultimate source of the poor segmentability of these languages lay in their restrictive phonotactics.

The present study seeks to test this proposal by applying the popular Unigram model~\cite{Brent99a,Goldwater07c,Goldwater09a} to artificially generated corpora. Artificial corpora are used here for the same reason that psycholinguistic experiments use carefully controlled stimuli rather than randomly sampled natural production: natural language corpora vary in a number of ways, arising from complex and potentially not understood relationships between interacting language properties. One example of a property that must be controlled is the word frequency distribution. It stands to reason that the word frequency distribution might differ between, e.g., Japanese and English for essentially syntactic reasons. Overt subjects are normally obligatory in English, so subject and object pronouns tend to make up a large noticable proportion of the total frequency. In contrast, discourse-salient subjects and objects are typically omitted in Japanese, so pronoun-like elements will likely make up a smaller proportion of the total frequency in Japanese corpora. As lexical segmentation models like the Unigram model embody particular assumptions about the word frequency distribution of a language, it stands to reason that this language property will affect segmentability by such a model, quite apart from the phonotactic properties. By generating artificial corpora, it is possible to vary the phonotactic properties while holding the word frequency distribution constant, controlling (to a certain extent) the impact of this additional factor. It is also makes possible to investigate the effect of the word frequency distribution. In fact, this is exactly what the present paper does.

Two artificial languages were created, representing extremes of phonotactic permissiveness and restrictiveness. Pseudo-Senufo is a strict CV grammar -- words must begin with a consonant and end with a vowel; no consonant-consonant or vowel-vowel sequences are permitted; and otherwise every possible sequence of $(CV)^*$ is grammatical. Pseudo-Berber has no phonotactic constraints whatsoever -- every possible sequence of consonants and vowels is grammatical. Both grammars were defined over the same segmental inventory, intended to represent a fairly minimal but cross-linguistically typical inventory (see Methods). In addition to the language manipulation (Berber vs. Senufo) and the word frequency distribution manipulation (described in more detail below), a final manipulation was to impose varying degrees of penalty on word length. This was done for a purely practical reason, to prevent overly long words from dominating the artificial lexicons; although the length penalty manipulation proved interesting.

\section{Methods}

\subsection{Phonotactic grammars}

The alphabet \Sigma used here was intended to represent a small but standard segmental inventory, including plain stops at 3 major places of articulation [ptk], corresponding fricatives [fsx] and nasals [mn\engma], basic liquids [lr] and glides [jw], and a standard 5-vowel inventory [aeiou]. These segments were assigned standard phonological feature values, e.g. [f] is [-son,+cont,+lab], while [a] is [+syl,+low]. In the implementation, strings are space-separated, e.g. `s t r i \engma'.

A \textit{phonotactic grammar} \mu over an alphabet \Sigma defines a probability distribution over \Sigma$^*$, i.e. it is a function $\mu : \Sigma^* \rightarrow [0,1]$ which assigns a probability to every string in \Sigma$^*$ (the set of finite strings over \Sigma). Here, the phonotactic grammar was defined using constraints stated over $n$-gram feature matrices, following the format of regular expressions. For example, the constraint `^[+syl]' penalizes words which begin with a vowel (often called \textsc{Onset} in the phonological literature). Each constraint is associated with a weight, and the `harmony' of a string is the weighted sum of its constraint violations. This defines a log-linear model, from which the probability of a string can be calculated exactly ~\cite{Hayes08a}. The maximum string length was 6 in the present paper.

The grammar of Pseudo-Senufo contains an \textsc{Onset} constraint `^[+syl]', a \textsc{NoCoda} constraint '[-syl]\$', a constraint banning consonant clusters `[-syl] [-syl]', and a constraint banning vowel hiatus `[+syl] [+syl]'. Each of these was given a very strong weight (-25), sufficient to categorically rule out violating forms, thereby enforcing a strict CV syllable structure. In addition, Pseudo-Senufo contained a \textsc{*Struct} constraint '[]' which penalized words according to how many segments they contained. This constraint was given a smaller weight (between -1 and -5), which was manipulated to control the average word length and lexical saturation.

The grammar of Pseudo-Berber contained only the \textsc{Struct} constraint, so that any string was a possible word, no matter how difficult to pronounce.

\section{Results}

this section describes what we found
%NOTE: \textit values are still running but seem to have converged, hence I included them to get an idea of what the complete table will look like
\begin{table*}[t!]
\begin{center}
\begin{tabular}{|c||c|cccccc|}
  \hline
  Language & *Struct & brent & zipf16 & zipf12 & linear & flat & point \\
  \hline
  Senufo & 1 & .85 & .65 & .83 & .66 & .58 & .13 \\
  Senufo & 2 & .87 & .65 & .83 & .78 & .76 & .13 \\
  Senufo & 3 & .83 & .50 & .71 & .91 & .91 & .13 \\
  Senufo & 4 & .83 & .50 & .68 & .95 & .96 & .13 \\
  Senufo & 5 & .83 & .50 & .67 & \textit{.95} & \textit{.97} & .14 \\
  \hline
  Berber & 1 & .93 & .85 & .91 & .70 & .62 & .20 \\
  Berber & 2 & .95 & .75 & .92 & .78 & .73 & .21 \\
  Berber & 3 & .95 & .62 & .87 & .93 & .91 & .20 \\
  Berber & 4 & .80 & .42 & .62 & .91 & .94 & .20 \\
  Berber & 5 & \textit{.59} & .33 & \textit{.45} & \textit{.85} & \textit{.91} & .19 \\
  \hline
\end{tabular}
\end{center}
\caption{\label{Results.}}
\end{table*}

\section{Discussion}

this section says what it all means

If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
can use the command \verb|\newcite| to get ``author (year)'' citations.


% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{acl}


\end{document}
